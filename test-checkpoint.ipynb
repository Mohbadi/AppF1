{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc23448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e8f9a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/33782/Desktop/Mémoire F1/df_final_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3edd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7805951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Supposons que vous avez déjà importé votre DataFrame df_final\n",
    "\n",
    "# Sélectionnez les colonnes à encoder\n",
    "columns_to_encode = [\"nationality\"]\n",
    "\n",
    "# Créez une instance de OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Adapter et transformer les colonnes sélectionnées\n",
    "encoded_data = encoder.fit_transform(df[columns_to_encode])\n",
    "\n",
    "# Créez un DataFrame avec les données encodées\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(columns_to_encode))\n",
    "\n",
    "# Supprimez les colonnes encodées du DataFrame d'origine\n",
    "df.drop(columns=columns_to_encode, inplace=True)\n",
    "\n",
    "# Concaténez le DataFrame original avec le DataFrame encodé\n",
    "df_final = pd.concat([df, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f0baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop(columns=\"positionOrder\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf029098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c6f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of columns excluding the one to move\n",
    "cols = [col for col in df_final.columns if col != 'Top 3 Finish']\n",
    "\n",
    "# Append the column to the end of the DataFrame\n",
    "df_final = df_final[cols + ['Top 3 Finish']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c2b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_final[(df_final[\"year\"] >= 2012) & (df_final[\"year\"] <= 2022)]\n",
    "test_df = df_final[(df_final[\"year\"] == 2023)]\n",
    "\n",
    "\n",
    "X_train = train_df[train_df.columns.tolist()[:-1]].values\n",
    "y_train = train_df['Top 3 Finish'].values\n",
    "\n",
    "\n",
    "X_test = test_df[train_df.columns.tolist()[:-1]].values\n",
    "y_test = test_df['Top 3 Finish'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a43549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f2d4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Créer un modèle SVM\n",
    "svm_model = SVC(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [8,9,10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Créer l'objet GridSearchCV\n",
    "svm_grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring=\"recall\", verbose=1, n_jobs=-1)\n",
    "\n",
    "# Exécuter la recherche d'hyperparamètres sur l'ensemble d'entraînement\n",
    "svm_grid_search.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "print(\"Best Parameters:\", svm_grid_search.best_params_)\n",
    "print(\"Best Score:\", svm_grid_search.best_score_)\n",
    "\n",
    "# Utiliser le meilleur modèle pour faire des prédictions sur l'ensemble de test\n",
    "best_svm_model = svm_grid_search.best_estimator_\n",
    "y_pred_svm = best_svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "class_report_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_svm)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_svm)\n",
    "print(\"\\nClassification Report:\\n\", class_report_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fc704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import optuna\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    C = trial.suggest_categorical('C', [12.24251805])\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear'])\n",
    "    gamma = trial.suggest_categorical('gamma', ['auto'])\n",
    "\n",
    "    # Créer un modèle SVM avec les hyperparamètres suggérés\n",
    "    svm_model = SVC(C=C, kernel=kernel, gamma=gamma, random_state=42)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    svm_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculer le score F1\n",
    "    score = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle SVM final\n",
    "best_svm_model = SVC(**best_params, random_state=42)\n",
    "best_svm_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_svm = best_svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "class_report_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_svm)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_svm)\n",
    "print(\"\\nClassification Report:\\n\", class_report_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import optuna\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    C = trial.suggest_float('C', 10, 20, log=True)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear'])\n",
    "    gamma = trial.suggest_categorical('gamma', ['auto'])\n",
    "\n",
    "    # Créer un modèle SVM avec les hyperparamètres suggérés\n",
    "    svm_model = SVC(C=C, kernel=kernel, gamma=gamma, random_state=42)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    svm_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculer le score F1\n",
    "    score = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle SVM final\n",
    "best_svm_model = SVC(**best_params, random_state=42)\n",
    "best_svm_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_svm = best_svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "class_report_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_svm)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_svm)\n",
    "print(\"\\nClassification Report:\\n\", class_report_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import optuna\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.1, log=True)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "\n",
    "    # Créer un modèle Gradient Boosting avec les hyperparamètres suggérés\n",
    "    gb_model = GradientBoostingClassifier(learning_rate=learning_rate,\n",
    "                                           n_estimators=n_estimators,\n",
    "                                           max_depth=max_depth,\n",
    "                                           min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf,\n",
    "                                           random_state=42)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    gb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculer le score F1\n",
    "    score = f1_score(y_test, y_pred_gb)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle Gradient Boosting final\n",
    "best_gb_model = GradientBoostingClassifier(**best_params, random_state=42)\n",
    "best_gb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_gb = best_gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "conf_matrix_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "class_report_gb = classification_report(y_test, y_pred_gb)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_gb)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_gb)\n",
    "print(\"\\nClassification Report:\\n\", class_report_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5c09c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 09:39:19,447] A new study created in memory with name: no-name-f9944fdd-2fe9-495b-9046-5f2383d13b28\n",
      "[I 2024-05-24 09:39:50,140] Trial 0 finished with value: 0.7797038800592305 and parameters: {'learning_rate': 0.04911196424566634, 'n_estimators': 122, 'max_depth': 5, 'min_samples_split': 20, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.7797038800592305.\n",
      "[I 2024-05-24 09:40:15,068] Trial 1 finished with value: 0.7943208939350216 and parameters: {'learning_rate': 0.04720323045901661, 'n_estimators': 70, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.7943208939350216.\n",
      "[I 2024-05-24 09:40:42,424] Trial 2 finished with value: 0.7879823093920845 and parameters: {'learning_rate': 0.04808830471294363, 'n_estimators': 106, 'max_depth': 4, 'min_samples_split': 17, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.7943208939350216.\n",
      "[I 2024-05-24 09:41:52,343] Trial 3 finished with value: 0.7984602577050023 and parameters: {'learning_rate': 0.016155435657070276, 'n_estimators': 162, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.7984602577050023.\n",
      "[I 2024-05-24 09:42:09,238] Trial 4 finished with value: 0.795983142871377 and parameters: {'learning_rate': 0.04216688792196581, 'n_estimators': 95, 'max_depth': 3, 'min_samples_split': 11, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.7984602577050023.\n",
      "[I 2024-05-24 09:42:43,898] Trial 5 finished with value: 0.7970278202031477 and parameters: {'learning_rate': 0.03252896447042469, 'n_estimators': 68, 'max_depth': 9, 'min_samples_split': 12, 'min_samples_leaf': 6}. Best is trial 3 with value: 0.7984602577050023.\n",
      "[I 2024-05-24 09:43:30,272] Trial 6 finished with value: 0.7927196910007162 and parameters: {'learning_rate': 0.020327318714343765, 'n_estimators': 198, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.7984602577050023.\n",
      "[I 2024-05-24 09:43:59,693] Trial 7 finished with value: 0.7813578382417349 and parameters: {'learning_rate': 0.05076945774434432, 'n_estimators': 128, 'max_depth': 4, 'min_samples_split': 7, 'min_samples_leaf': 9}. Best is trial 3 with value: 0.7984602577050023.\n",
      "[I 2024-05-24 09:45:46,643] Trial 8 finished with value: 0.7982753813757428 and parameters: {'learning_rate': 0.010454724541702973, 'n_estimators': 200, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.7984602577050023.\n",
      "[I 2024-05-24 09:46:07,626] Trial 9 finished with value: 0.8005857960214481 and parameters: {'learning_rate': 0.028075089163461433, 'n_estimators': 71, 'max_depth': 5, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.8005857960214481.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.028075089163461433, 'n_estimators': 71, 'max_depth': 5, 'min_samples_split': 8, 'min_samples_leaf': 2}\n",
      "Best Score: 0.8005857960214481\n",
      "\n",
      "Accuracy: 0.8636363636363636\n",
      "\n",
      "Confusion Matrix:\n",
      " [[333  41]\n",
      " [ 19  47]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92       374\n",
      "           1       0.53      0.71      0.61        66\n",
      "\n",
      "    accuracy                           0.86       440\n",
      "   macro avg       0.74      0.80      0.76       440\n",
      "weighted avg       0.88      0.86      0.87       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "import optuna\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.1, log=True)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "\n",
    "    # Créer un modèle Gradient Boosting avec les hyperparamètres suggérés\n",
    "    gb_model = GradientBoostingClassifier(learning_rate=learning_rate,\n",
    "                                           n_estimators=n_estimators,\n",
    "                                           max_depth=max_depth,\n",
    "                                           min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf,\n",
    "                                           random_state=42)\n",
    "\n",
    "    # Utiliser la validation croisée stratifiée\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    f1_scores = cross_val_score(gb_model, X_train_scaled, y_train_resampled, cv=skf, scoring='f1')\n",
    "\n",
    "    # Calculer la moyenne des scores F1\n",
    "    mean_f1_score = np.mean(f1_scores)\n",
    "\n",
    "    return mean_f1_score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle Gradient Boosting final\n",
    "best_gb_model = GradientBoostingClassifier(**best_params, random_state=42)\n",
    "best_gb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_gb = best_gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "conf_matrix_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "class_report_gb = classification_report(y_test, y_pred_gb)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_gb)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_gb)\n",
    "print(\"\\nClassification Report:\\n\", class_report_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3883629",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m learning_curve\n\u001b[0;32m      2\u001b[0m train_sizes, train_scores, valid_scores \u001b[38;5;241m=\u001b[39m learning_curve(best_gb_model,\n\u001b[0;32m      3\u001b[0m                                                              X_train_scaled,\n\u001b[0;32m      4\u001b[0m                                                              y_train_resampled,\n\u001b[0;32m      5\u001b[0m                                                              train_sizes\u001b[38;5;241m=\u001b[39m[i\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m11\u001b[39m)])\n\u001b[1;32m----> 7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_sizes, train_scores\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m,marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_sizes, valid_scores\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidaiton\u001b[39m\u001b[38;5;124m'\u001b[39m,marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "train_sizes, train_scores, valid_scores = learning_curve(best_gb_model,\n",
    "                                                             X_train_scaled,\n",
    "                                                             y_train_resampled,\n",
    "                                                             train_sizes=[i/10 for i in range(1,11)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f973aef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(train_sizes, train_scores.mean(axis=1), label='training',marker='.', color='blue')\n",
    "plt.plot(train_sizes, valid_scores.mean(axis=1), label='validaiton',marker='.', color='orange')\n",
    "plt.title(f\"Learning curve of the  model\")\n",
    "plt.legend(loc='best')\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0177eca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import optuna\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1, log=True)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 8)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    \n",
    "    # Créer un modèle Gradient Boosting avec les hyperparamètres suggérés\n",
    "    gb_model = GradientBoostingClassifier(learning_rate=learning_rate,\n",
    "                                           n_estimators=n_estimators,\n",
    "                                           max_depth=max_depth,\n",
    "                                           min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf,\n",
    "                                           random_state=42)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    gb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculer le score F1\n",
    "    score = f1_score(y_test, y_pred_gb)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=25)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle Gradient Boosting final\n",
    "best_gb_model = GradientBoostingClassifier(**best_params, random_state=42)\n",
    "best_gb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_gb = best_gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "conf_matrix_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "class_report_gb = classification_report(y_test, y_pred_gb)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_gb)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_gb)\n",
    "print(\"\\nClassification Report:\\n\", class_report_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb535a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import optuna\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.001, 1, log=True)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 12)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    \n",
    "    # Créer un modèle Gradient Boosting avec les hyperparamètres suggérés\n",
    "    gb_model = GradientBoostingClassifier(learning_rate=learning_rate,\n",
    "                                           n_estimators=n_estimators,\n",
    "                                           max_depth=max_depth,\n",
    "                                           min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf,\n",
    "                                           random_state=42)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    gb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculer le score F1\n",
    "    score = f1_score(y_test, y_pred_gb)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=75)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle Gradient Boosting final\n",
    "best_gb_model = GradientBoostingClassifier(**best_params, random_state=42)\n",
    "best_gb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_gb = best_gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "conf_matrix_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "class_report_gb = classification_report(y_test, y_pred_gb)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_gb)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_gb)\n",
    "print(\"\\nClassification Report:\\n\", class_report_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import optuna\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [0.0019772333924089505])\n",
    "    n_estimators = trial.suggest_int('n_estimators', 191, 191)\n",
    "    max_depth = trial.suggest_int('max_depth', 9, 9)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 8, 8)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 5, 5)\n",
    "    \n",
    "    # Créer un modèle Gradient Boosting avec les hyperparamètres suggérés\n",
    "    gb_model = GradientBoostingClassifier(learning_rate=learning_rate,\n",
    "                                           n_estimators=n_estimators,\n",
    "                                           max_depth=max_depth,\n",
    "                                           min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf,\n",
    "                                           random_state=42)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    gb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculer le score F1\n",
    "    score = f1_score(y_test, y_pred_gb)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle Gradient Boosting final\n",
    "best_gb_model = GradientBoostingClassifier(**best_params, random_state=42)\n",
    "best_gb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_gb = best_gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "conf_matrix_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "class_report_gb = classification_report(y_test, y_pred_gb)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_gb)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_gb)\n",
    "print(\"\\nClassification Report:\\n\", class_report_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b89065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(best_gb_model,X_train_scaled, y_train_resampled, cv=5, scoring='f1')\n",
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a1746",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5caee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report,accuracy_score\n",
    "import optuna\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [0.0010794024887915957])\n",
    "    n_estimators = trial.suggest_int('n_estimators', 194, 194)\n",
    "    max_depth = trial.suggest_int('max_depth', 7, 7)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 2)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 5, 5)\n",
    "    \n",
    "    # Créer un modèle Gradient Boosting avec les hyperparamètres suggérés\n",
    "    gb_model = GradientBoostingClassifier(learning_rate=learning_rate,\n",
    "                                           n_estimators=n_estimators,\n",
    "                                           max_depth=max_depth,\n",
    "                                           min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf,\n",
    "                                           random_state=42)\n",
    "\n",
    "    # Évaluer le modèle avec validation croisée\n",
    "    scores = cross_val_score(gb_model,X_train_scaled, y_train_resampled, cv=5, scoring='f1')\n",
    "\n",
    "    # Calculer le score moyen de la validation croisée\n",
    "    score = np.mean(scores)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle Gradient Boosting final\n",
    "best_gb_model = GradientBoostingClassifier(**best_params, random_state=42)\n",
    "best_gb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_gb = best_gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "conf_matrix_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "class_report_gb = classification_report(y_test, y_pred_gb)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_gb)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_gb)\n",
    "print(\"\\nClassification Report:\\n\", class_report_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc57b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tracer les learning curves\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_gb_model, X_train_scaled, y_train_resampled, cv=10, n_jobs=-1, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), scoring='f1')\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import optuna\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 50)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2'])\n",
    "    \n",
    "    # Créer un modèle Random Forest avec les hyperparamètres suggérés\n",
    "    rf_model = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                       max_depth=max_depth,\n",
    "                                       min_samples_split=min_samples_split,\n",
    "                                       min_samples_leaf=min_samples_leaf,\n",
    "                                       max_features=max_features,\n",
    "                                       random_state=42)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    rf_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculer le score F1\n",
    "    score = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle Random Forest final\n",
    "best_rf_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_rf_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_rf = best_rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "class_report_rf = classification_report(y_test, y_pred_rf)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_rf)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_rf)\n",
    "print(\"\\nClassification Report:\\n\", class_report_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ac386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import optuna\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2', None])\n",
    "    \n",
    "    # Créer un modèle DecisionTreeClassifier avec les hyperparamètres suggérés\n",
    "    dt_model = DecisionTreeClassifier(max_depth=max_depth,\n",
    "                                      min_samples_split=min_samples_split,\n",
    "                                      min_samples_leaf=min_samples_leaf,\n",
    "                                      max_features=max_features,\n",
    "                                      random_state=42)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    dt_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred_dt = dt_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculer le score F1\n",
    "    score = f1_score(y_test, y_pred_dt)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle DecisionTreeClassifier final\n",
    "best_dt_model = DecisionTreeClassifier(**best_params, random_state=42)\n",
    "best_dt_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_dt = best_dt_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "class_report_dt = classification_report(y_test, y_pred_dt)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_dt)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_dt)\n",
    "print(\"\\nClassification Report:\\n\", class_report_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d8ce6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import optuna\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    iterations = trial.suggest_int('iterations', 50, 500)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.3)\n",
    "    depth = trial.suggest_int('depth', 4, 10)\n",
    "    l2_leaf_reg = trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10)\n",
    "    \n",
    "    # Créer un modèle CatBoost avec les hyperparamètres suggérés\n",
    "    cb_model = CatBoostClassifier(iterations=iterations,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  depth=depth,\n",
    "                                  l2_leaf_reg=l2_leaf_reg,\n",
    "                                  verbose=0,\n",
    "                                  random_state=42)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    cb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred_cb = cb_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculer le score F1\n",
    "    score = f1_score(y_test, y_pred_cb)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle CatBoost final\n",
    "best_cb_model = CatBoostClassifier(**best_params, verbose=0, random_state=42)\n",
    "best_cb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_cb = best_cb_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_cb = accuracy_score(y_test, y_pred_cb)\n",
    "conf_matrix_cb = confusion_matrix(y_test, y_pred_cb)\n",
    "class_report_cb = classification_report(y_test, y_pred_cb)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_cb)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_cb)\n",
    "print(\"\\nClassification Report:\\n\", class_report_cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1012233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(best_cb_model,X_train_scaled, y_train_resampled, cv=5, scoring='f1')\n",
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a4262",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import optuna\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    iterations = trial.suggest_int('iterations', 50, 500)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.001, 1,log=True)\n",
    "    depth = trial.suggest_int('depth', 3, 12)\n",
    "    l2_leaf_reg = trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10)\n",
    "    \n",
    "    # Créer un modèle CatBoost avec les hyperparamètres suggérés\n",
    "    cb_model = CatBoostClassifier(iterations=iterations,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  depth=depth,\n",
    "                                  l2_leaf_reg=l2_leaf_reg,\n",
    "                                  verbose=0,\n",
    "                                  random_state=42)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    cb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred_cb = cb_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculer le score F1\n",
    "    score = f1_score(y_test, y_pred_cb)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle CatBoost final\n",
    "best_cb_model = CatBoostClassifier(**best_params, verbose=0, random_state=42)\n",
    "best_cb_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_cb = best_cb_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_cb = accuracy_score(y_test, y_pred_cb)\n",
    "conf_matrix_cb = confusion_matrix(y_test, y_pred_cb)\n",
    "class_report_cb = classification_report(y_test, y_pred_cb)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_cb)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_cb)\n",
    "print(\"\\nClassification Report:\\n\", class_report_cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e117ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(best_cb_model,X_train_scaled, y_train_resampled, cv=5, scoring='f1')\n",
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185988fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import optuna\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.001, 1,log=True)\n",
    "    \n",
    "    # Créer un modèle AdaBoost avec les hyperparamètres suggérés\n",
    "    ab_model = AdaBoostClassifier(n_estimators=n_estimators,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  random_state=42)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    ab_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred_ab = ab_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculer le score F1\n",
    "    score = f1_score(y_test, y_pred_ab)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle AdaBoost final\n",
    "best_ab_model = AdaBoostClassifier(**best_params, random_state=42)\n",
    "best_ab_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_ab = best_ab_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_ab = accuracy_score(y_test, y_pred_ab)\n",
    "conf_matrix_ab = confusion_matrix(y_test, y_pred_ab)\n",
    "class_report_ab = classification_report(y_test, y_pred_ab)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_ab)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_ab)\n",
    "print(\"\\nClassification Report:\\n\", class_report_ab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c8490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import optuna\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    n_estimators = trial.suggest_int('n_estimators', 160, 160)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.15826735517631804, 0.15826735517631804,log=True)\n",
    "    algorithm = trial.suggest_categorical('algorithm', ['SAMME', 'SAMME.R'])\n",
    "    # Vous pouvez ajouter d'autres hyperparamètres à optimiser ici, comme base_estimator, etc.\n",
    "\n",
    "    # Créer un modèle AdaBoost avec les hyperparamètres suggérés\n",
    "    ab_model = AdaBoostClassifier(n_estimators=n_estimators,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  algorithm=algorithm,\n",
    "                                  random_state=42)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    ab_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred_ab = ab_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculer le score F1\n",
    "    score = f1_score(y_test, y_pred_ab,zero_division=1)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=3)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle AdaBoost final\n",
    "best_ab_model = AdaBoostClassifier(**best_params, random_state=42)\n",
    "best_ab_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_ab = best_ab_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_ab = accuracy_score(y_test, y_pred_ab)\n",
    "conf_matrix_ab = confusion_matrix(y_test, y_pred_ab)\n",
    "class_report_ab = classification_report(y_test, y_pred_ab)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_ab)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_ab)\n",
    "print(\"\\nClassification Report:\\n\", class_report_ab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e7e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(best_ab_model,X_train_scaled, y_train_resampled, cv=5, scoring='f1')\n",
    "print(np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569bd3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import optuna\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Définir les classificateurs de base pour le stacking\n",
    "base_classifiers = [\n",
    "    ('rf', RandomForestClassifier(random_state=42)),\n",
    "    ('ab', AdaBoostClassifier(random_state=42)),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser pour le classificateur de stacking\n",
    "    meta_algorithm = trial.suggest_categorical('meta_algorithm', ['logistic', 'rf'])\n",
    "    meta_max_depth = trial.suggest_int('meta_max_depth', 3, 10)\n",
    "    \n",
    "    # Créer un modèle de stacking avec les hyperparamètres suggérés\n",
    "    stack_model = StackingClassifier(estimators=base_classifiers,\n",
    "                                     final_estimator=AdaBoostClassifier(random_state=42),\n",
    "                                     cv=5)\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    stack_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred_stack = stack_model.predict(X_test_scaled)\n",
    "\n",
    "    # Calculer le score F1\n",
    "    score = f1_score(y_test, y_pred_stack)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle de stacking final\n",
    "meta_algorithm = best_params['meta_algorithm']\n",
    "meta_max_depth = best_params['meta_max_depth']\n",
    "final_estimator = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "stack_model = StackingClassifier(estimators=base_classifiers,\n",
    "                                 final_estimator=final_estimator,\n",
    "                                 cv=5)\n",
    "stack_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_stack = stack_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy_stack = accuracy_score(y_test, y_pred_stack)\n",
    "conf_matrix_stack = confusion_matrix(y_test, y_pred_stack)\n",
    "class_report_stack = classification_report(y_test, y_pred_stack)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy_stack)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix_stack)\n",
    "print(\"\\nClassification Report:\\n\", class_report_stack)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Normaliser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def objective(trial):\n",
    "    # Définir les hyperparamètres à optimiser\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1, log=True)\n",
    "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 5)\n",
    "    num_neurons = trial.suggest_int('num_neurons', 10, 100)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(Dense(num_neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    model.fit(X_train_scaled, y_train_resampled, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Faire des prédictions sur l'ensemble de test\n",
    "    y_pred_proba = model.predict(X_test_scaled)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    # Calculer le score F1\n",
    "    score = f1_score(y_test, y_pred)\n",
    "\n",
    "    return score\n",
    "\n",
    "# Créer l'étude Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=75)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Utiliser les meilleurs paramètres pour créer le modèle final\n",
    "model = Sequential()\n",
    "model.add(Dense(best_params['num_neurons'], activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dropout(best_params['dropout_rate']))\n",
    "\n",
    "for _ in range(best_params['num_hidden_layers']):\n",
    "    model.add(Dense(best_params['num_neurons'], activation='relu'))\n",
    "    model.add(Dropout(best_params['dropout_rate']))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate'])\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_scaled, y_train_resampled, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Évaluer les performances du meilleur modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Afficher les performances\n",
    "print(\"\\nAccuracy:\", accuracy)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
